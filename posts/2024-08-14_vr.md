General VR stack simplified:

[VR app]
↕ VR API
[VR runtime]
↕ stuff between runtime and HMD incl. sensors protocols
[VR HMD and controllers]

All common VR apps use VR APIs with VR libraries or runtimes implementing them, which abstract away complexities and differences of HMDs and controllers, presenting them in uniform convenient manner and taking care of stuff which app developers won't be happy to bother with. You can see VR APIs as analog for 3D graphics APIs, and VR runtimes as analog for implementations of these APIs, some being specific to single hardware family (like in graphics drivers from GPUs vendors), others more broad/modular (like Mesa). Each HMD usually has "perfect support" in specific vendor-supplied VR runtime, but can be also supported by alternate FOSS project (currently all effort is converging to Monado project becoming like "Mesa for VR"), also there are "translation layers" between common APIs and other "bridges" allowing to use apps developed against an API with HMDs which don't have this exact API support in vendor runtime, but have one for other API which is simpler to develop against then to reverse engineer HMD stuff (like all those D3D<->Vulkan wrappers).

Note: VR APIs don't handle 3D rendering, rendering is done using existing 3D graphics APIs, VR APIs are used to obtain data which affects rendering, such as spatial positions of cameras for projections and FOV, and possibly to pass rendered frames for applying transformations required for proper viewing through HMD optics and/or merging with something rendered by VR runtime itself (such as generic VR control HUD).

Note: in tech world terms "XR" (extended) or "AR" (augmented) are now preferred to "VR"; because VR as understood by most "HMD with double display and sensors and possibly some controllers for pointing/moving" is just a specific usecase requiring solution of more general tasks:

- "track spatial position of devices, user body parts and other objects in real world" (including tracking via cameras, which belongs to computer vision domain)
- "generate image projections based on position of displays, observers eyes, optics between them"

Another common non-VR usecase is "magic window", when virtual environment is "projected" on phone display (not placed in some "phone HMD") adjusting to its spatial position. Besides, there are developments in area of volumetric displays.

Note: VR runtimes are userpace things and all VR-specific stuff is computed in userspace (well, for GPUs on most platforms 3D graphics APIs are also implemented in userpace, with kernel parts of GPU drivers doing only basic stuff). If runtime has modular architecture allowing to add support for different HMDs, modules are usually called drivers.

There are now 3 relevant cross-vendor VR APIs:

- **Khronos OpenXR**. 2017-here to stay. THE industry standard, cross-vendor, cross-platform, "like Vulkan in VR world". Used by most new AAA games ( https://pimax.com/blogs/blogs/steam-vr-vs-openxr-which-runtime-is-best state of adoption in AAA games as of spring 2024). Supported by all major VR runtimes, including:
  - Monado (by Collabora). Cross-platform FOSS runtime supporting many HMDs from different vendors (for now support for all common consumer HMDs seems not yet mature compared to proprietary runtimes provided by vendors, with positional tracking missing or partially implemented, but there is a lot of progress happening), "like Mesa in VR world"
  - SteamVR. Proprietary, works on Windows and Linux (still in beta, since 2016). Primarily supports Valve Index HMD, HTC Vive and other 3rd party HMDs which use same hardware protocols known as "lighthouse" designed for Valve VR ecosystem. Has modular architecture with several other official and 3rd party "SteamVR drivers" (note that Linux support in runtime itself doesn't mean Linux support in any driver, which can depend on some Windows-only stuff, though "lighthouse" driver does support Linux, though not without issues).
  - Oculus runtime. Supports Oculus/Meta HMDs. Windows-only.
  - PimaxXR. "Unofficial officially-promoted" OpenXR runtime for Pimax HMDs (that are otherwise supported via "official" SteamVR driver, both seem to depend on some proprietary Windows-only Pimax stuff)
- **Valve OpenVR**. 2015. Was created before OpenXR as first major attempt to standartize PC VR and untie it from Oculus/Meta. Used by most "first wave" PC VR AAA games (before OpenXR adoption). Implementations:
  - SteamVR, official implementation of it, and only of it before OpenXR adoption, "SteamVR" name is often misused as name of the API.
  - OpenComposite, OpenVR-to-OpenXR translation layer (but it seems it can't run Alyx flawlessly yet)
- **W3C WebXR**. 2018. Yes, web developers can now do it too. Notably used by web VR video players. Implementations:
  - Chrome for Android (seems to work only on some phones via Google proprietary stuff, see ARCore, and yes, it renders eye projections on phone screen for viewing with "Cardboard VR" devices).
  - Chrome desktop (seems to work only on Windows, via OpenXR and other APIs)
  - *not* implemented in Firefox as of now (which is especially sad given that its deprecated "prototype" WebVR originated from Mozilla, also there was Firefox Reality VR web browser which is now dead)
  - Igalia Wolvic. Ok, Firefox Reality is not really dead, https://wolvic.com/en/faq/ https://github.com/Igalia/wolvic/issues/1394 need to further clarify which platforms does it target and whether it can be run on some platform with FOSS VR stack
  - there's also https://github.com/immersive-web/webxr-polyfill which provides some level of support on phones via Sensors APIs which are supported on all common web browsers on mobile devices allowing web app to access sensors readings, but getting it to work seems really hard.

Notable vendor-specific APIs:

- **Google Cardboard**/Daydream/**ARCore**. 2014/2016-2019/2018. Started with Cardboard, implemented in now open-source Cardboard library for Android, notably used by PhoneVR allowing to use phone as PC VR HMD, also there's still mostly useless Cardboard demo app in Play store. Then Google tried to push Daydream as next generation phone VR platform, but made it proprietary, limited to flagship phones and killed it soon. Then it created ARCore which is again proprietary and limited to flagship phones (including Pixel of course), notably used by Chrome for Android for WebXR. Though it seems to also make use of Cardboard library for setting up projections. And "Cardboard VR" also became synonym to "phone VR". There's also ambigious "Google VR" term which seems to mean either Daydream runtime, or Google-provided VR SDK allowing to develop against both Cardboard and Daydream. In general phone VR seem to be currently constrained by absence of mature FOSS OpenXR implementation relying on screen and sensors of the phone itself, Monado which already has some Android support may become the one. Most standalone HMDs like Oculus Quest also run Android(-based software), but with their own proprietary VR stacks tied to their hardware.
- Oculus/Meta OVR aka Oculus API. 2012-soon? Oculus were pioneers of consumer VR and are still leading HMD producer, unfortunately bought by Meta in early days and locked down their ecosystem. After adopting OpenXR announced deprecation of their API.
- Microsoft WMR (Windows Mixed Reality). 2015-2024? Currently in process of being killed, but few good HMDs were mass produced and are now very cheaply available used (HP Reverb G2), useable for OpenXR/OpenVR apps on Windows via SteamVR wich has wmr backend communicating with WMR runtime, making it act as OpenXR/OpenVR-WMR translation layer (and there is also Monado driver for WMR HMDs which doesn't rely on Microsoft WMR runtime, as WMR spec appears to cover not just API but also hardware protocols, but Monado WMR driver tracking seems to be subpar for now)

# HMDs

Wired PC VR HMD connectivity: most PC VR HMDs still use separate USB/HDMI/power cables, even if "bound" together. There was a push from Nvidia for "all-in-one" USB-C connector and cable using USB DP alt mode for video out and USB PD for power delivery, which they called VirtualLink (it was just subset of modern USB, nothing proprietary), but quickly abandoned it, "VirtualLink" USB-C port was only present in some RTX 20xx cards. "All-in-one" USB-C is also present in some newer AMD cards, including RX 7000 series. HMDs which support single USB-C cable are still hard to find. Standalone Oculus Quest supports Oculus Link USB-C cable, but it sends video via proprietary Oculus software using H.264/H.265 compression.

Wired PC VR HMDs all use DP/HDMI for video out, but for for sensors data there is no cross-vendor protocol standard. Closest one is probably Valve "lighthouse" which is used by multiple manufacturers ("minor independent" manufactures tend to target Valve ecosystem).

Most PC VR HMDs are designed for "room-scale VR" with "outside-in" tracking which means they require placing some stationary beacons for positional tracking/6DoF. "Inside-out" tracking is available in WMR HMDs, some Pimax HMDs ( https://pimax.com/blogs/blogs/pose-tracking-methods-outside-in-vs-inside-out-tracking-in-vr ) and standalone Oculus Quest.

Some specific HMDs which are of interest to me:

- Valve Index: "reference" PC VR HMD
- Bigscreen Beyond: smallest&lightest, also designed for SteamVR (lighthouse protocols)
- Xreal Air: AR glasses with ~50 degrees FOV, supported by Monado driver

# Using phone as PC VR

It seems that currently the only mature FOSS (partially FOSS, works on Linux) solution is https://github.com/alvr-org/ALVR (PC-side "server") with https://github.com/alvr-org/PhoneVR (phone-side "client"), relies on SteamVR (server is implemented as SteamVR driver). https://www.youtube.com/watch?v=_5k9htTdpuI
https://github.com/WiVRn/WiVRn and https://gitlab.freedesktop.org/monado/electric-maple rely on Monado (2nd had some attention from Collabora itself), but currently seem to target only standalone HMDs like Oculus Quest.

# Linux VR desktop

I'm currently interested in "simple" solution with single desktop "projected" on VR surface, like a giant screen floating in space in front of user, initially positioned on HMD focal plane, with possibility to re-center it in case it drifts.

- https://github.com/galister/wlx-overlay-s
- https://github.com/SimulaVR/Simula

# 3DoF VR videos

This is only type of VR video which is currently affordable to produce, it is filmed by double camera with wide angle lens (180 degree, or 360 degree using 2 such double cameras and subsequent "stitching" of 2 180degree videos). Perception is perfect only as long as head movements are restricted to positions in which eyes positions match cameras (only pitch/nodding movements, following the central vertical line of video), but in practice feeling of depth is good for most people when looking around "naturally".

To be more precise about video format, there are different options, but almost all use "SBS EQR" (side-by-side equirectangular).

Most websites with VR videos currently seem to use "Delight XR" video player which relies on browser WebXR support and can be recognized by common UI and \<dl8-video\> tag in code: https://delight-vr.com/documentation/dl8-video/

Solutions for local playback:
- https://github.com/mysterion/aframe-vr-player . It's a "self-sufficient" webapp, which can be seen from phone VR perspective as smart hack to leverage good support of WebXR in Chrome on Android devices with ARCore, without bothering with Android development
- proprietary DeoVR and SKYBOX VR players are often mentioned, both seem to have given up phone VR in favor of standalone HMDs like Oculus Quest, DeoVR is also "VR video publishing platform" trying to stay on the tip of progress and has interesting blog posts and community discussions, including 6DoF tech developments.
  - https://deovr.com/blog/54-passthrough-the-next-frontier-of-the-virtual-reality-experience
  - https://deovr.com/blog/92-6dof-volumetric-experiences-at-deovr

# 6DoF VR videos

Currently most impressive thing I've seen is Light fields demo from Google team https://augmentedperception.github.io/deepviewvideo/ , they also released demo app in Steam with static 6DoF images https://store.steampowered.com/app/771310/Welcome_to_Light_Fields/

There's also Pseudoscience VR player popular in reddit 6DoF community, which seems to "guess" depth for 3DoF videos and hidden parts of objects.

# Physical-physiological FAQ

- *when I look at point in space in VR, where do my eyes actually look? Is it identical to looking at same point if VR scene was recreated IRL? Is light from that point identical?* VR is not true holography/light field reconstruction. Physically eye "looking at something" means it turns in certain direction and accomodates its lens curvature for certain distance to catch all light coming from a point at that distance into matching single point on retina (producing sharpest image of respective object). VR HMDs are currently "unifocal", meaning that HMD lens are transforming light from displays in such way that it's effectively same as light from bigger displays placed at fixed distance ~2-3m from eyes (not collimated/"from infinity" as some think, this choice of distance probably comes from expectation about at how far objects will people most often look in VR, or maybe from it matching some "average" accomodation curvature). Feeling of depth mostly comes from stereoscopic parralax differences, but accomodation also matters, though not that much for most people, and beyond 6m it's almost same, "infinity focus". In VR most peoples brains will quickly "learn" to always keep focus on that ~2-3m "focal plane" which gives sharpest image of any object no matter how close or far it is as perceived via stereoscopic parralax convergence mechanism, but it will feel as slight discomfort and "something is different", known as VAC (vergence-accomodation conflict). There are attempts to create "varifocal" HMDs, but nothing is really available yet (see Creal).

# Other notes and links

- https://lvra.gitlab.io/ "Linux VR Adventures Wiki"
- https://danielmalshriky.medium.com/glossary-of-extended-reality-8bdddc903a2d
- https://delight-vr.com/xr-glossary/
- Steam Link "remote gaming" app has version for Oculus Quest which allows to use it as PC VR HMD with PC running SteamVR
- https://github.com/santeri3700/vive-pro-2-on-linux
- Doom 3 (2004) is one of first 3D FPS games which felt "quite real", there are VR developments by community ("DOOM-3-BFG-VR" aka "Fully Possessed" mod https://github.com/NPi2Loup/DOOM-3-BFG-VR https://www.reddit.com/r/virtualreality/comments/180dxgn/doom_3_bfg_pcvr_new_update/ , RBDOOM-3-BFG fork https://github.com/Codes4Fun/RBDOOM-3-BFG , both seem to target OpenVR API), and modern flagship phones hardware is probably powerful enough to render it in VR, there's some GLES port https://github.com/glKarin/com.n0n3m4.diii4a https://github.com/glKarin/com.n0n3m4.diii4a/tree/master/Q3E/src/main/jni/doom3bfg (also based on RBDOOM-3-BFG, which seems to be "main" community port of 2012 Doom 3 BFG Edition "remaster" engine, as opposed to dhewm3 which is "main" community port of original 2004 Doom 3 engine); perhaps it would be interesting task to bring it to phone VR when Monado for Android matures
- OpenHMD is "frozen" FOSS project which intended to provide VR runtime with support for several HMDs, including Oculus Rift from DK1 to S, with its own open API http://openhmd.net/doxygen/0.1.0/openhmd_8h.html , now it's supported by Monado as driver for those HMDs
